<?xml version="1.0" encoding="UTF-8"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="11FAB29B-E950-4C08-8DA0-B9C90881B438">
            <Title>Project Proposal</Title>
            <Text>Proposal: How “weak” does a “weak learner” need to be?

In this project, the investigators set out to understand how “strong” a “weak learner” must be to do successful ADA boosting. Using the ADA boosting procedure, we intend to investigate how strongly the “weak learner” must be trained to be successful machine learning algorithm

Procedure:
The methods implemented in this paper are as such: a fully connect CNN used as the weak learner using the (blank) data set. The hyper parameters we will be using is the training data size used to learn with and the number of weak learners used in the ada process.

Goals and expectations:
We intend to find the threshold at which a weak learner has been trained enough to demonstrate sufficient accuracy to be a useable algorithm to classify the data set. This means changing both the depth of training and number of weak learners used in the classification process. </Text>
        </Document>
        <Document ID="F260DA7C-6CC5-4CB7-83F7-307B9880B6F8">
            <Title>text</Title>
            <Text>Introduction:
	Machine learning is an exciting field of research, partly due to the wealth of techniques and plethora of questions to be answered. When navigating such an open field, to make pertinent and interesting contributions we are prioritizing conciseness. As such, we will be asking ourselves: how  “weak” is a “weak learner”. Using the ADA boosting algorithm, we will be tuning hyper parameters of the algorithm to find a the weakness threshold at which algorithm converges on a meaningful classification algorithm. For our weak learner, we will be using a fully-connected CNN against the MNIST data set. These are standard resources used in the field, which will help us create clear statements about our finding, as we will not get lost in the implementation of a more exotic technique or data set. 



Motivation: when using ADA boosting, a set of “weak learners” is chained together to make a more successful algorithm. While the rest of the algorithm is clearly defined, the definition of what a “weak learner” is vague and up to user. In our project we hope to quantify this idea by ranging our hyper parameters of the “weak learner” and observing how these values affect the classification power of the overall algorithm. By doing this, we should be able to see at what thresholds these hyper parameters create a successful algorithm.


Evaluation:
There are two important evaluation criteria to be responsible for: the error of each “weak learner” and the strength of the overall algorithm. For the individual “weak learners” we will use a traditional classification error as prescribed by the ADA algorithm. As for the overall effectiveness of the algorithm, this is a more subtle question. Due to the nature of question we are looking to answer, we are interested the full range that the overall classification accuracy of algorithm takes. In particular, we are interested in how changing the tuning parameters turns a “weak” algorithm (one with above random guessing but not completely accurate) to a “strong” algorithm (one with a high classification accuracy). A successful implantation of the answering our question will show a smooth transition from “weak” to “strong” algorithm. 

Resources:
As described above, we will implanting a fully-connected Convolutional Neural Network and training it with the MNIST data set. Due to the relative simplicity of this implementation, there is many options to take. Depending on the difficulty of implementation, we will either be using the standard machine learning package PyTorch, or the open source code from Michael Nielsen’s web book Neural Networks and Deep Learning. The latter is attractive in that we will have access to every line of code, making it easier to modify for our specifications. The simplicity of our algorithm may make it possible to not use a more powerful library. 

Contributions:
The following are the major steps in the project:
Preprocessing of the MNIST data
Postprocessing of the data (graphing and analysis)
Creating of the overall code of our implementation 
Using that code to create the data for the report
</Text>
        </Document>
    </Documents>
</SearchIndexes>